{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear SVM...\n",
      "[LibSVM]Training KNN...\n",
      "Training Random Forest...\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 76500\n",
      "[LightGBM] [Info] Number of data points in the train set: 19461, number of used features: 300\n",
      "[LightGBM] [Info] Start training from score -1.651200\n",
      "[LightGBM] [Info] Start training from score -0.968555\n",
      "[LightGBM] [Info] Start training from score -1.500308\n",
      "[LightGBM] [Info] Start training from score -1.582368\n",
      "Training XGBoost...\n",
      "\n",
      "Results for Linear SVM\n",
      "Training Accuracy: 0.9169621293869791\n",
      "Test Accuracy: 0.9151180913559526\n",
      "Precision: 0.916191323030186\n",
      "Recall: 0.9151180913559526\n",
      "F1 Score: 0.9151479313002661\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.86      0.88      1575\n",
      "     Class 1       0.88      0.93      0.91      3176\n",
      "     Class 2       0.96      0.91      0.93      1915\n",
      "     Class 3       0.94      0.93      0.93      1675\n",
      "\n",
      "    accuracy                           0.92      8341\n",
      "   macro avg       0.92      0.91      0.92      8341\n",
      "weighted avg       0.92      0.92      0.92      8341\n",
      "\n",
      "\n",
      "Results for KNN\n",
      "Training Accuracy: 0.9287292533785519\n",
      "Test Accuracy: 0.905526915237981\n",
      "Precision: 0.90854978504857\n",
      "Recall: 0.905526915237981\n",
      "F1 Score: 0.9061639209769159\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.81      0.91      0.86      1575\n",
      "     Class 1       0.91      0.90      0.91      3176\n",
      "     Class 2       0.96      0.89      0.92      1915\n",
      "     Class 3       0.93      0.94      0.93      1675\n",
      "\n",
      "    accuracy                           0.91      8341\n",
      "   macro avg       0.90      0.91      0.91      8341\n",
      "weighted avg       0.91      0.91      0.91      8341\n",
      "\n",
      "\n",
      "Results for Random Forest\n",
      "Training Accuracy: 0.9998972303581523\n",
      "Test Accuracy: 0.9249490468768733\n",
      "Precision: 0.9262730301869462\n",
      "Recall: 0.9249490468768733\n",
      "F1 Score: 0.9249479700826195\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.86      0.90      1575\n",
      "     Class 1       0.89      0.95      0.92      3176\n",
      "     Class 2       0.96      0.92      0.94      1915\n",
      "     Class 3       0.96      0.94      0.95      1675\n",
      "\n",
      "    accuracy                           0.92      8341\n",
      "   macro avg       0.93      0.92      0.93      8341\n",
      "weighted avg       0.93      0.92      0.92      8341\n",
      "\n",
      "\n",
      "Results for LightGBM\n",
      "Training Accuracy: 0.9996403062535327\n",
      "Test Accuracy: 0.9366982376213884\n",
      "Precision: 0.9368522945555983\n",
      "Recall: 0.9366982376213884\n",
      "F1 Score: 0.9366831965906458\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.90      0.91      1575\n",
      "     Class 1       0.92      0.94      0.93      3176\n",
      "     Class 2       0.95      0.93      0.94      1915\n",
      "     Class 3       0.96      0.96      0.96      1675\n",
      "\n",
      "    accuracy                           0.94      8341\n",
      "   macro avg       0.94      0.93      0.94      8341\n",
      "weighted avg       0.94      0.94      0.94      8341\n",
      "\n",
      "\n",
      "Results for XGBoost\n",
      "Training Accuracy: 0.9999486151790761\n",
      "Test Accuracy: 0.9404148183671023\n",
      "Precision: 0.9404840309804026\n",
      "Recall: 0.9404148183671023\n",
      "F1 Score: 0.9403746369480187\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.91      0.92      1575\n",
      "     Class 1       0.93      0.95      0.94      3176\n",
      "     Class 2       0.96      0.93      0.94      1915\n",
      "     Class 3       0.96      0.96      0.96      1675\n",
      "\n",
      "    accuracy                           0.94      8341\n",
      "   macro avg       0.94      0.94      0.94      8341\n",
      "weighted avg       0.94      0.94      0.94      8341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import re\n",
    "\n",
    "# Load normalized data\n",
    "data = pd.read_csv(r\"C:\\Users\\Dell\\Downloads\\normalized_data.csv\")\n",
    "\n",
    "# Download FastText model\n",
    "fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Some useful functions for FastText\n",
    "def get_average_fasttext(tokens_list, model, generate_missing=False, k=300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [model[word] if word in model else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [model[word] if word in model else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_fasttext_embeddings(model, tokens, generate_missing=False):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embeddings.append(get_average_fasttext(token.split(), model, generate_missing=generate_missing))\n",
    "    return embeddings\n",
    "\n",
    "# Create tokens column from the description\n",
    "data['tokens'] = data['description'].apply(lambda x: ' '.join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "# Split data into train and test sets (70:30 ratio)\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "# FastText embedding\n",
    "X_train_fasttext = get_fasttext_embeddings(fasttext_model, train_data['tokens'])\n",
    "X_test_fasttext = get_fasttext_embeddings(fasttext_model, test_data['tokens'])\n",
    "\n",
    "y_train = train_data['label']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Convert embeddings to NumPy arrays\n",
    "X_train_fasttext = np.array(X_train_fasttext)\n",
    "X_test_fasttext = np.array(X_test_fasttext)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear SVM\": SVC(kernel='linear', verbose=True),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(objective='multiclass', num_class=len(np.unique(y_train))),\n",
    "    \"XGBoost\": xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)))\n",
    "}\n",
    "\n",
    "# Define label_dict with appropriate class labels and names\n",
    "label_dict = {0: 'Class 0', 1: 'Class 1', 2: 'Class 2', 3: 'Class 3'}  # Adjust according to your actual class labels and names\n",
    "\n",
    "# Initialize results dictionary to store accuracies and reports\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    model.fit(X_train_fasttext, y_train)\n",
    "    \n",
    "    if model_name in [\"LightGBM\", \"XGBoost\"]:\n",
    "        y_pred_train = np.argmax(model.predict_proba(X_train_fasttext), axis=1)\n",
    "        y_pred_test = np.argmax(model.predict_proba(X_test_fasttext), axis=1)\n",
    "    else:\n",
    "        y_pred_train = model.predict(X_train_fasttext)\n",
    "        y_pred_test = model.predict(X_test_fasttext)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    test_classification_report = classification_report(y_test, y_pred_test, target_names=label_dict.values())\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"Training Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Classification Report\": test_classification_report\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for model, result in results.items():\n",
    "    print(f\"\\nResults for {model}\")\n",
    "    print(\"Training Accuracy:\", result[\"Training Accuracy\"])\n",
    "    print(\"Test Accuracy:\", result[\"Test Accuracy\"])\n",
    "    print(\"Precision:\", result[\"Precision\"])\n",
    "    print(\"Recall:\", result[\"Recall\"])\n",
    "    print(\"F1 Score:\", result[\"F1 Score\"])\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(result[\"Classification Report\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
