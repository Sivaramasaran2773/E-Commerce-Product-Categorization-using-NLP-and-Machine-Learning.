{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8385040,"sourceType":"datasetVersion","datasetId":4986970}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nimport re\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tqdm import tqdm\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Load normalized data\ndata = pd.read_csv(\"/kaggle/input/ecomnorm/normalized_data.csv\")\n\n# Load BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n\n# Set batch size\nbatch_size = 32\n\n# Modify get_bert_embeddings function to accept batch_size parameter\ndef get_bert_embeddings(sentences, max_length=128, batch_size=batch_size):\n    embeddings = []\n    for i in tqdm(range(0, len(sentences), batch_size)):\n        batch_sentences = sentences[i:i+batch_size]\n        tokenized_texts = [tokenizer.tokenize(\"[CLS] \" + sentence + \" [SEP]\")[:max_length] for sentence in batch_sentences]\n        input_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n        input_ids = [ids + [0] * (max_length - len(ids)) for ids in input_ids]  # Padding\n        input_ids = torch.tensor(input_ids).to(device)\n        with torch.no_grad():\n            outputs = bert_model(input_ids)\n        batch_embeddings = outputs.last_hidden_state\n        avg_embeddings = torch.mean(batch_embeddings, dim=1).cpu().numpy()\n        embeddings.append(avg_embeddings)\n    return np.concatenate(embeddings, axis=0)\n\n# Create tokens column from the description\ndata['tokens'] = data['description'].apply(lambda x: ' '.join(re.findall(r'\\w+', x.lower())))\n\n# Split data into train and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n\n# Get BERT embeddings\nX_train_bert = get_bert_embeddings(train_data['tokens'])\nX_test_bert = get_bert_embeddings(test_data['tokens'])\n\ny_train = train_data['label']\ny_test = test_data['label']\n\n# Models\nmodels = {\n    \"Linear SVM\": SVC(kernel='linear', verbose=True),\n    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"LightGBM\": LGBMClassifier(),\n    \"XGBoost\": xgb.XGBClassifier()\n}\n\nresults = {}\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f\"Training {model_name}...\")\n    model.fit(X_train_bert, y_train)\n    print(f\"Finished training {model_name}.\")\n\n    print(f\"Evaluating {model_name}...\")\n    y_train_pred = model.predict(X_train_bert)\n    y_test_pred = model.predict(X_test_bert)\n\n    # Calculate accuracies\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n    test_accuracy = accuracy_score(y_test, y_test_pred)\n\n    # Precision, recall, F1-score\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n\n    # Classification report\n    classification_report_text = classification_report(y_test, y_test_pred)\n\n    results[model_name] = {\n        \"Train Accuracy\": train_accuracy,\n        \"Test Accuracy\": test_accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1-score\": f1,\n        \"Classification Report\": classification_report_text\n    }\n\n    print(f\"Results for {model_name}:\")\n    print(\"Training Accuracy:\", train_accuracy)\n    print(\"Test Accuracy:\", test_accuracy)\n    print(\"Precision:\", precision)\n    print(\"Recall:\", recall)\n    print(\"F1-score:\", f1)\n    print(\"Classification Report:\")\n    print(classification_report_text)\n    print(\"=\" * 50)\n\n# Organize and print final results\nprint(\"Final Results:\")\nfor model_name, result in results.items():\n    print(model_name)\n    print(\"Training Accuracy:\", result[\"Train Accuracy\"])\n    print(\"Test Accuracy:\", result[\"Test Accuracy\"])\n    print(\"Precision:\", result[\"Precision\"])\n    print(\"Recall:\", result[\"Recall\"])\n    print(\"F1-score:\", result[\"F1-score\"])\n    print(\"Classification Report:\")\n    print(result[\"Classification Report\"])\n    print(\"=\" * 50)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T06:50:27.877048Z","iopub.execute_input":"2024-05-21T06:50:27.877653Z","iopub.status.idle":"2024-05-21T07:00:18.045142Z","shell.execute_reply.started":"2024-05-21T06:50:27.877621Z","shell.execute_reply":"2024-05-21T07:00:18.044313Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a2166a2167c4d1eb7a21bde8cb212c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d18129a5fc448a6b750ee81ecf41af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9980d7480144d21b5cae70287f874a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ea5799675b94296adaadfe8b8199e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d4ba7c8fe6b4b3bb8fc3937c6383105"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/609 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n100%|██████████| 609/609 [03:21<00:00,  3.02it/s]\n100%|██████████| 261/261 [01:29<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training Linear SVM...\n[LibSVM].........................................*..............................*....*\noptimization finished, #iter = 74674\nobj = -1599.627559, rho = -6.190021\nnSV = 2067, nBSV = 1522\n.................*............*....*\noptimization finished, #iter = 33130\nobj = -629.930654, rho = 0.483407\nnSV = 971, nBSV = 556\n.....*...*\noptimization finished, #iter = 8481\nobj = -250.686538, rho = -4.218498\nnSV = 519, nBSV = 237\n.......................*..............*.*\noptimization finished, #iter = 38658\nobj = -992.095123, rho = 3.047693\nnSV = 1383, nBSV = 932\n...........*.....*\noptimization finished, #iter = 16505\nobj = -707.099564, rho = 1.071838\nnSV = 1079, nBSV = 726\n......*..*\noptimization finished, #iter = 8774\nobj = -358.669956, rho = -3.867346\nnSV = 652, nBSV = 371\nTotal nSV = 4418\nFinished training Linear SVM.\nEvaluating Linear SVM...\nResults for Linear SVM:\nTraining Accuracy: 0.9457376291043625\nTest Accuracy: 0.9220716940414818\nPrecision: 0.9221036333739958\nRecall: 0.9220716940414818\nF1-score: 0.9220013783240619\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.87      0.88      1575\n           1       0.91      0.93      0.92      3176\n           2       0.94      0.92      0.93      1915\n           3       0.94      0.95      0.95      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.92      0.92      0.92      8341\nweighted avg       0.92      0.92      0.92      8341\n\n==================================================\nTraining KNN...\nFinished training KNN.\nEvaluating KNN...\nResults for KNN:\nTraining Accuracy: 0.9210729150608911\nTest Accuracy: 0.8947368421052632\nPrecision: 0.8960341555448137\nRecall: 0.8947368421052632\nF1-score: 0.8947263959059941\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.83      0.86      1575\n           1       0.87      0.91      0.89      3176\n           2       0.96      0.90      0.93      1915\n           3       0.89      0.93      0.91      1675\n\n    accuracy                           0.89      8341\n   macro avg       0.90      0.89      0.89      8341\nweighted avg       0.90      0.89      0.89      8341\n\n==================================================\nTraining Random Forest...\nFinished training Random Forest.\nEvaluating Random Forest...\nResults for Random Forest:\nTraining Accuracy: 1.0\nTest Accuracy: 0.8984534228509771\nPrecision: 0.9023943487063879\nRecall: 0.8984534228509771\nF1-score: 0.8982661064428366\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.80      0.86      1575\n           1       0.84      0.94      0.89      3176\n           2       0.94      0.92      0.93      1915\n           3       0.95      0.89      0.92      1675\n\n    accuracy                           0.90      8341\n   macro avg       0.91      0.89      0.90      8341\nweighted avg       0.90      0.90      0.90      8341\n\n==================================================\nTraining LightGBM...\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172592 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 195840\n[LightGBM] [Info] Number of data points in the train set: 19461, number of used features: 768\n[LightGBM] [Info] Start training from score -1.651200\n[LightGBM] [Info] Start training from score -0.968555\n[LightGBM] [Info] Start training from score -1.500308\n[LightGBM] [Info] Start training from score -1.582368\nFinished training LightGBM.\nEvaluating LightGBM...\nResults for LightGBM:\nTraining Accuracy: 0.9985098401932069\nTest Accuracy: 0.9203932382208369\nPrecision: 0.920656683312859\nRecall: 0.9203932382208369\nF1-score: 0.9203395420901552\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.87      0.89      1575\n           1       0.90      0.93      0.92      3176\n           2       0.95      0.93      0.94      1915\n           3       0.93      0.94      0.94      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.92      0.92      0.92      8341\nweighted avg       0.92      0.92      0.92      8341\n\n==================================================\nTraining XGBoost...\nFinished training XGBoost.\nEvaluating XGBoost...\nResults for XGBoost:\nTraining Accuracy: 1.0\nTest Accuracy: 0.924709267473924\nPrecision: 0.9250928181436066\nRecall: 0.924709267473924\nF1-score: 0.9246720945999153\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.89      1575\n           1       0.91      0.94      0.92      3176\n           2       0.95      0.93      0.94      1915\n           3       0.94      0.94      0.94      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.93      0.92      0.92      8341\nweighted avg       0.93      0.92      0.92      8341\n\n==================================================\nFinal Results:\nLinear SVM\nTraining Accuracy: 0.9457376291043625\nTest Accuracy: 0.9220716940414818\nPrecision: 0.9221036333739958\nRecall: 0.9220716940414818\nF1-score: 0.9220013783240619\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.87      0.88      1575\n           1       0.91      0.93      0.92      3176\n           2       0.94      0.92      0.93      1915\n           3       0.94      0.95      0.95      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.92      0.92      0.92      8341\nweighted avg       0.92      0.92      0.92      8341\n\n==================================================\nKNN\nTraining Accuracy: 0.9210729150608911\nTest Accuracy: 0.8947368421052632\nPrecision: 0.8960341555448137\nRecall: 0.8947368421052632\nF1-score: 0.8947263959059941\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.83      0.86      1575\n           1       0.87      0.91      0.89      3176\n           2       0.96      0.90      0.93      1915\n           3       0.89      0.93      0.91      1675\n\n    accuracy                           0.89      8341\n   macro avg       0.90      0.89      0.89      8341\nweighted avg       0.90      0.89      0.89      8341\n\n==================================================\nRandom Forest\nTraining Accuracy: 1.0\nTest Accuracy: 0.8984534228509771\nPrecision: 0.9023943487063879\nRecall: 0.8984534228509771\nF1-score: 0.8982661064428366\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.80      0.86      1575\n           1       0.84      0.94      0.89      3176\n           2       0.94      0.92      0.93      1915\n           3       0.95      0.89      0.92      1675\n\n    accuracy                           0.90      8341\n   macro avg       0.91      0.89      0.90      8341\nweighted avg       0.90      0.90      0.90      8341\n\n==================================================\nLightGBM\nTraining Accuracy: 0.9985098401932069\nTest Accuracy: 0.9203932382208369\nPrecision: 0.920656683312859\nRecall: 0.9203932382208369\nF1-score: 0.9203395420901552\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.87      0.89      1575\n           1       0.90      0.93      0.92      3176\n           2       0.95      0.93      0.94      1915\n           3       0.93      0.94      0.94      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.92      0.92      0.92      8341\nweighted avg       0.92      0.92      0.92      8341\n\n==================================================\nXGBoost\nTraining Accuracy: 1.0\nTest Accuracy: 0.924709267473924\nPrecision: 0.9250928181436066\nRecall: 0.924709267473924\nF1-score: 0.9246720945999153\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.91      0.88      0.89      1575\n           1       0.91      0.94      0.92      3176\n           2       0.95      0.93      0.94      1915\n           3       0.94      0.94      0.94      1675\n\n    accuracy                           0.92      8341\n   macro avg       0.93      0.92      0.92      8341\nweighted avg       0.93      0.92      0.92      8341\n\n==================================================\n","output_type":"stream"}]}]}